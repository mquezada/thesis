\chapter{Background}
\label{chapter:background}

In this chapter, we describe the particularities of Twitter, and introduce
relevant techniques used in this thesis.
%
In particular, we describe a form of supervised learning: classification; and of
unsupervised learning: clustering.
%
We also describe the use of {\em word embeddings}, in particular, the current
usage of neural network-based word embeddings, such as {\em word2vec} and {\em
fastText}.

\section{Twitter and Social Media}

In this section, we briefly describe Twitter: how it works, its social
capabilities and the relevant endpoints of its API to retrieve newsworthy
messages.

\subsection{Twitter Platform}

As mentioned in the Introduction, the collective of information published in
Internet-based applications, such as microblogging platforms, blogs, wikis,
etc., is called {\em social media}~\cite{kaplan2010users}.


We used the microblogging service Twitter~\cite{twitter} as our data source and
platform of study.
%
Twitter is a service that lets users publish short 280-character messages, called
{\em tweets}, which are broadcast to the user's {\em followers}.
%
A follower is simply another user of the platform, who can subscribe to other
users (to {\em follow} them), and see their tweets in an interface called the
{\em timeline}.
%
The timeline is a list of tweets; the timeline was originally displayed in
reverse-chronological order, that is, the newest tweets were shown at the
beginning.
%
Currently, the timeline uses a custom ordering based on the user
characteristics, with the objective of encouraging the user to interact more
with the platform.

Social interaction between users in Twitter can occur not only from the follower
relation, but by the content of the messages.
%
A user can {\em mention} another user in a tweet by prepending their username
with a {\tt @} symbol.
%
A {\em retweet} is the action of forwarding a message from a user to the current
user's followers.
%
A {\em quote} is a retweet with an additional message from the user committing
the action.
%
Additionally, tweets can be {\em replied to} and this action creates a {\em
thread} that can be explored in the interface.
%
These ways to interact show the social aspect of Twitter.

\subsection{Twitter API}\label{sec:twitterapi}

Anyone with a Twitter account can sign up for a developer account and use the
Twitter API. The API allows users to retrieve publicly available information,
such as public tweets and public Twitter profiles.

To retrieve tweets, there are two main endpoints in the API. They are described
below.

\begin{itemize}
    \item {\bf Standard Search API}\footnote{\raggedright See
    \url{https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html}
    (Accessed: August 24, 2019)}. 
    %
    This endpoint allows us to retrieve tweets given search terms.
    %
    According to Twitter, this endpoint focuses ``{\em on relevance and not
    completeness}'', meaning that it is not an exhaustive search and will not
    retrieve all the available tweets relevant to a query. 
    %
    Another aspect of this endpoint is that it is supported to return at most
    7-day old tweets, but in some cases we observed that it returned even older
    tweets.
    %
    Furthermore, as the original tweet is included when the retrieved tweet is a
    retweet, we can obtain information about much older tweets this way.

    \item {\bf Filter real-time Tweets}\footnote{\raggedright Previously called
    {\em Streaming API}. See
    \url{https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/post-statuses-filter.html}
    (Accessed: August 24, 2019)}.
    %
    This endpoint allows us to retrieve tweets in real time by filtering the
    stream by certain keywords.
    %
    In the previous version of the API, Twitter stated that this endpoint
    retrieves at most 1\% of the full, complete stream.
    %
    However, the tweets do not always match those distributed by the full
    stream, called the {\em PowerTrack API}\footnote{Previously called the {\em
    Firehose}. See
    \url{https://developer.twitter.com/en/docs/tweets/filter-realtime/overview/powertrack-api.html}
    (Accessed: August 24, 2019)}, as with some filters the distributions
    differed significantly~\cite{morstatter2013sample}.
    
\end{itemize}

Chapter~\ref{chapter:data} describes how we use the Standard
Search API to retrieve the tweets used in the following chapters.



\section{Supervised learning: Classification}

The basic premise of learning from data lies in the use of a set of observations
to uncover an underlying process\footnote{This section is broadly based on the
book {\em Learning from Data} from Abu-Mustafa et
al.~\cite{Abu-Mostafa:2012:LD:2207825}}. 
%
More formally, a way to see this is by finding a function that optimizes a
certain score based on the available data. 
%
This function can be seen as an approximation of the real, unknown function
that describes the process that generates the data.

%%

When the {\em training data} (the available data) contains explicit examples of
what the correct output should be, then we are within the supervised learning
setting.
%
In this setting, classification is the process of assigning a certain {\em label}
to an observation or data point, in order to categorize it into a certain class
or category.
%
A {\em classifier} is a particular instance of this process, and a classifier
can be {\em trained} from a set of previously labeled observations to set its
appropriate parameters.
%
The output of a classifier applied to an observation is the label predicted for 
that observation (Figure~\ref{background:fig:clf-ex}).
%
Formally, a classifier is a function $h: \mathcal{D} \rightarrow \{0, 1, \ldots,
n\}$ from a domain $\mathcal{D}$ to a set of classes, or class labels, which
tries to approximate to the {\em real} function $f$.
%
A {\em binary classifier} is a function restricted to only two labels, $h:
\mathcal{D} \rightarrow \{0, 1\}$.
%
The training process of a classifier corresponds to finding a function $h$ from
a {\em hypothesis set} $\mathcal{H}$, which contains all possible functions
based on the selected {\em model}.
%
For example, $\mathcal{H}$ would be all quadratic functions $h(x) = ax^2 + bx +
c$, and the goal is to find the correct set of parameters $(a, b, c)$ that make
$h$ fit the training data.
%
An error measure that is commonly used is the squared error: $e(h) =
(h(\vect{x}) - f(\vect{x}))^2$.
%
As we do not know $f$, the error measure is only an approximation of the real
error, which is called {\em in-sample error}, while the real error is called
{\em out-of-sample error}.
% 
There are models that estimate bounds for which the in-sample error can
approximate the out-of-sample error.
%
In the binary setting, the Vapnikâ€“Chervonenkis (VC) dimension~\cite{Vapnik2015}
is an estimation of how well a function $h$ from a hypothesis set can fit a set
of data points, based on the characteristics of $\mathcal{H}$ and the number of
data points.

%
\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{figures/background/classification-example}
    \caption{Example of classification as a decision boundary.}
    \label{background:fig:clf-ex}
\end{figure}%


\paragraph{Linear models.} The linear model is the family of functions (or
hypothesis set) defined as $h(\vect{x}) = \vect{w}^\customT\vect{x}$, where
$\vect{w} \in \mathbb{R}^\textit{d}, \textit{d} > 0$ is a vector of weights, and
$\vect{x}\in \mathbb{R}^\textit{d}$ is an observation. 
%
The {\em perceptron} is defined as $h(\vect{x}) =
\text{sign}(\vect{w}^\customT\vect{x})$, which is the foundation of several
linear models and neural network models. 
%
The perceptron uses a hard threshold on the signal $s =
\vect{w}^\customT\vect{x}$ by taking the sign of $s$ in order to perform linear
classification.
%
{\em Linear regression} outputs a real value by using the signal $s$ as the
output, $h(\vect{x}) = \vect{w}^\customT\vect{x}$.
%
{\em Logistic regression} restricts the output to the range $[0, 1]$, making
it interpretable as a probability: $h(\vect{x}) =
\theta(\vect{w}^\customT\vect{x})$, where $\theta$ is the so-called logistic
function, $\theta(s) = \frac{e^s}{1 + e^s}$.

%

There are several linear-based models, which we will not mention in this brief
summary, but most of them can be studied
online\footnote{\url{https://scikit-learn.org/stable/modules/linear_model.html}
(Accessed: April 8, 2019)}.

%%

There are different learning algorithms in each case. 
%
For the perceptron, there is the {\em Perceptron Learning Algorithm} and the
{\em Pocket Algorithm}~\cite{Abu-Mostafa:2012:LD:2207825}. 
%
For the linear regression, a exact analytic expression can be derived by
minimizing the in-sample error, $E_{\text{in}}(h) = \frac{1}{N} \sum_{n=1}^{N}
(h(\vect{x}_n) - y_n)^2$, where $y_n = f(\vect{x}_n)$ is the label of the $n$-th
observation.
%
For logistic regression, the {\em gradient descent algorithm} provides a way to
find a local optimum for the in-sample error $E_{\text{in}}(h)$, where the
in-sample error is defined by the {\em logistic loss}.
%
The logistic loss function is defined as $\ell: x \mapsto (1 +
\mathit{e}^{-x})$.
% 
In other words, the in-sample error for logistic regression is
$E_{\text{in}}(\vect{w}) = \frac{1}{N}\sum_{n=1}^N
\ell(-y_n\vect{w}^\text{T}\vect{x}_n)$.



%%

Gradient descent is a general technique for minimizing a twice-differentiable
function. 
%
It is performed by computing the gradient of the in-sample error $\nabla
E_{\text{in}}(h)$, and ``moving'' the weight vector according to the direction
of the gradient, $\vect{w}_{t+1} = \vect{w}_t - \eta\nabla E_{\text{in}}(h)$.
%
There are several strategies to stop the iteration $t$, such as doing it until
the change is small, a fixed number of times, when the error is small enough, or
a combination of the above.
%
The magnitude of the movement is given by the parameter $\eta$, or {\em learning
rate}, usually a fixed value.

\paragraph{Evaluation strategies.}

The standard workflow of training a classifier involves splitting the available
data into a {\em training set} and a {\em test set}. 
%
This is known as {\em holdout}.
%
However, in order to not {\em overfit} a model to the test set, and therefore
having an overly optimistic estimate of the performance of the classifier, an
extra step involves using a {\em validation set}\footnote{Or {\em development
set}, as known in the Deep Learning community.}.
%
That is, the common methodology is to split the data into three sets: train,
validation and test.
%
The validation set is used to tune the {\em hyperparameters} of the classifier,
by training using the training set and evaluating the classifier using the
validation set.
%
When all the parameters have been set and learned, the final estimate of
$E_{\text{out}}(\vect{h})$ is given by the test set.

%%

Another common technique is to reduce the uncertainty by making use of all the
available data. One of the methods to do so is called {\em cross-validation}.
%
Cross-validation splits the data set into $k$ folds, training with the data
resulting from appending $k-1$ folds and testing with the remainder, and then 
repeating the process using another fold to test.
%
This reduces the variance of the estimates and is useful for estimating the
hyperparameters instead of a unique validation set.

%%

\paragraph{Evaluation metrics.} 
%
The most common metric to evaluate the effectiveness of a classifier is the {\em
accuracy}, the amount of correctly classified observations over the total amount of
observations.

\begin{equation}
    \text{Accuracy} = \frac{\text{\#(correctly\ classified\ observations)}}{N}
\end{equation}

Where $N$ is the total amount of observations being classified.

Accuracy does not account for an imbalance in the class distribution.
%
For example, when there are two classes, and they are split into 99\% class $0$
and 1\% class $1$, it is then easy to obtain an accuracy of $0.99$ by
classifying everything as class $0$.
%
The notion of {\em Precision} and {\em Recall} take into account two types of
error of a classifier.

\begin{equation}
    \text{Precision}_c = \frac{\text{\#(observations\ correctly\ classified\ as\ } c \text{)}}{ \text{\#(observations\ classified\ as\ } c \text{)} }
\end{equation}

\begin{equation}
    \text{Recall}_c = \frac{\text{\#(observations\ correctly\ classified\ as\ } c \text{)}}{ \text{\#(observations\ of\ class\ } c\text{)} } 
\end{equation}

The related errors are the {\em false positives} (when an observation is
incorrectly classified as the target class) or FP, the {\em false negatives}
(when an observation is incorrectly classified as something that is not the
target class), or FN. 
%
When an observation is correctly classified as the target class, we say it is a
{\em True Positive}, TP, and when it is correctly classified not as the target
class, a {\em True Negative}, or TN.
%
Now we can re-write the definitions of accuracy Accuracy, precision $P$, and recall
$R$:

\begin{align}
    \text{Accuracy} &= \frac{\cTP + \cTN}{\cTP + \cFP + \cFN + \cTN} \\
    P &= \frac{\cTP}{\cTP + \cFP} \\
    R &= \frac{\cTP}{\cTP + \cFN} 
\end{align}

A single measure that trades off precision versus recall is the {\em F-score} or
{\em F measure}, which is the weighted harmonic mean of precision and recall:

\begin{equation}
    F = \frac{1}{\alpha\frac{1}{P} + (1-\alpha)\frac{1}{R}} = \frac{(\beta^2+1)PR}{\beta^2P+R} \text{\ \ where\ \ } \beta^2 = \frac{1-\alpha}{\alpha}
\end{equation}

where $\alpha \in [0, 1]$ and thus $\beta^2 \in [0, \infty]$. 
%
The default {\em balanced F measure} equally weights precision and recall, which
means $\alpha=1/2$ or $\beta=1$.
%
It is commonly written as $F_1$.
%
In this case, the formula translates to:

\begin{equation}
    F_1 = \frac{2PR}{P+R}
\end{equation}

%%

The {\em confusion matrix} is a specific table layout that shows the performance
of a classifier.
%
Each row of the matrix represents the instances of the actual class, while each
column represents the instances in the predicted classes.
%
When there are two classes, the cells of the matrix represent the TP, FP, FN,
and TN of the classifier results.
%
Table~\ref{tab:background:example-conf-matrix} shows an example of a confusion
matrix with three classes.

%%

Another concept sometimes used in the evaluation is the {\em ROC
curve}\footnote{Or Receiver Operating Characteristics curve.}. 
%
The ROC curve plots the {\em true positive rate}, {\em sensitivity} or simply
recall, against the {\em false positive rate} or $(1 - \mathit{specificity})$.
%
The false positive rate is given by $\cFP/(\cFP + \cTN)$.
%
The true positive and false positive rates are computed based on different
thresholds imposed on the decision boundary of a classifier, for example, at the
threshold of a logistic regression classifier when deciding if an observation
belongs to one or the other class.


\begin{table}[]
    \centering
    \begin{tabular}{@{}llll@{}}
    \toprule
    Actual / Predicted & \textit{Class 0} & \textit{Class 1} & \textit{Class 2} \\ \midrule
    \textit{Class 0}   & \textbf{10}      & 4                & 6                \\
    \textit{Class 1}   & 2                & \textbf{14}      & 0                \\
    \textit{Class 2}   & 4                & 23               & \textbf{9}       \\ \bottomrule
    \end{tabular}
    \caption{Example of confusion matrix for a classifier with three classes.}
    \label{tab:background:example-conf-matrix}
    \end{table}

\section{Unsupervised learning: Clustering}

Clustering algorithms group a set of documents into subsets or {\em
clusters}\footnote{This section is broadly based on the book {\em Introduction
to Information Retrieval} from Manning et al.~\cite{manning2010introduction}}.
%
The goal of the algorithms is to produce clusters that are internally coherent,
while very different from each other.
%
Clustering is the most common form of unsupervised learning, when we do not have
the exact labels corresponding to each observation. 
%
In this case, it is the features in the data which will correlate to the labels.
%
Our goal is to replicate the distinction that a human observer would impose on
the data.


\paragraph{K-Means.} 
%
K-Means is a partitional clustering algorithm which assigns each observation to
exactly one cluster.
%
The objective of K-Means~\cite{lloyd1982least} is to minimize the average
squared Euclidean distance of documents from their cluster
centers~\cite{manning2010introduction}.
%
A cluster center is defined as the mean, or {\em centroid} $\vect{\mu}$ of the
observations in a cluster $\omega$:

$$\vect{\mu}(\omega) = \frac{1}{|\omega|} \sum_{\vect{x} \in \omega} \vect{x}$$

K-Means aims to minimize the {\em sum of the squared error}, or
SSE\footnote{Also called {\em Residual Sum of Squares}, or RSS.}, the squared
distance of each vector from its centroid summed over all vectors:

$$ \text{SSE}_k = \sum_{\vect{x} \in \omega_k} |\vect{x} - \vect{\mu}(\omega_k)|^2 $$

$$ \text{SSE} = \sum_{k = 1}^K \text{SSE}_k, $$

where $K$ is the total amount of clusters, and $\omega_k$ is the $k$-th cluster.

Initially, the algorithm selects the initial $K$ centroids randomly.
%
Then it moves the centers around in space in order to minimize SSE.
%
Iteratively it reassigns the observations to the cluster with the closest
centroid, and recomputes each centroid based on the current members of its
cluster. 

The algorithm is listed in Algorithm~\ref{alg:kmeans}.

\begin{algorithm}
    \caption{{\scshape K-Means}$(\{\vect{x}_1, \ldots, \vect{x}_N\}, K)$}
    \label{alg:kmeans}
    \begin{algorithmic}[1]
        \STATE $(\vect{s}_1, \vect{s}_2, \ldots, \vect{s}_K) \gets$ {\scshape SelectRandomSeeds}$(\{\vect{x}_1, \ldots, \vect{x}_N\}, K)$
        \FOR{$k \gets 1\ \TO\ K$} \STATE $\vect{\mu}_k \gets \vect{s}_k$ \ENDFOR
        \WHILE{stopping\ criterion\ has\ not\ been\ met}
            \FOR{$k \gets 1\ \TO\ K$}
                \STATE $\omega_k \gets \{\}$
            \ENDFOR
            \FOR{$n \gets 1\ \TO\ N$}
                \STATE $j \gets \text{arg} \min_{j'} |\vect{\mu}_{j'} - \vect{x}_n|$
                \STATE $\omega_j \gets \omega_j \cup \{\vect{x}_n\}$
            \ENDFOR
            \FOR{$k \gets 1\ \TO\ K$}
                \STATE $\vect{\mu}_k \gets \frac{1}{\omega_k}\sum_{\vect{x}\in\omega_k}\vect{x}$
            \ENDFOR
        \ENDWHILE
        \RETURN $\{\vect{\mu}_1, \ldots, \vect{\mu}_K\}$
    \end{algorithmic}
\end{algorithm}

K-Means has some considerations to be taken into account:

\begin{itemize}
\item {\bf Termination condition.} 
%
We can apply one of the following termination conditions.
%
(i) a fixed number of iterations $I$ has been completed; 
%
in this case, the quality of the clusters can be compromised because an
insufficient number of iterations.
%
(ii) the assignment of data points to clusters does not change between
iterations; 
%
this can yield a good clustering solution, except for a bad local minimum.
%
(iii) the centroids $\vect{\mu}_k$ does not change between iterations;
%
this is equivalent to the assignment not changing.
%
(iv) terminate when SSE falls below a certain threshold.
%
(v) terminate when the decrease of SSE falls below a certain threshold, meaning
that we are close to convergence.
%
A common strategy is to combine criteria, such as a maximum number of
iterations, a threshold on the value or decreasing rate of SSE, etc.

\item {\bf How to choose the initial seeds.} 
%
Effective heuristics for choosing the initial seeds include:
%
(i) excluding outliers from the seed set;
%
(ii) trying multiple starting points and choosing the best clustering (in terms
of SSE);
%
(iii) obtain the seeds from other methods, such as hierarchical clustering.

\item {\bf Selecting the number of clusters $K$.}
%
The following is one of the most popular heuristics to choose the number of
clusters $K$ on K-Means, when there is no previous conception of what the
number should be.
%
For every value of $K$ in a chosen range, compute K-Means $t$ times (e.g. $t =
10$), each with a different initialization of seeds, and take the minimum value
of SSE of the $t$ runs.
%
We can then inspect the values of SSE for every $K$ and find the ``knee'' or
``elbow'' in the curve: the point where successive decreases in SSE become
noticeably smaller.
%
This is known as the {\em elbow method} for selecting the number of clusters.
%
Another criterion imposes a penalty on the number of clusters.
%
For example, the elbow method could suggest different values of $K$, and we
would prefer a smaller value.
%
The criterion uses a weighting factor $\lambda$ on the number of clusters: $K =
\text{arg min}_K [\text{SSE}_{\text{min}}(K) + \lambda K]$.
%
This is similar to the regularization factor in classification.
%
The value of $\lambda$ can be derived empirically or by use of a value 
shown to be effective in the domain of study.
\end{itemize}

\paragraph{Evaluation of clustering.} 
%
Criteria on clustering quality involve two perspectives: internal and external
evaluation.
%
Internal evaluation assesses the characteristics that objective functions in
clustering try to attain, such as having high intra-cluster similarity (elements
within a cluster are similar), and low inter-clustering similarity (elements
from different clusters are dissimilar).
%
On the other hand, when we have a {\em gold standard}, or a set of classes
corresponding to the ``real'' clusters, we can perform an external evaluation by
comparing our clustering solution to the gold standard.

%%

In the case of K-Means, the value of the SSE is itself an internal criterion,
because we are validating our solution in terms of the intra-cluster similarity.
%
One can use both intra and inter-cluster similarity to judge the quality of the
clustering. There are other measures such as the Silhouette
value~\cite{rousseeuw1987silhouettes}, and the Davies-Bouldin
index~\cite{davies1979cluster}.

%%

In this section we will describe the following external evaluation measures for
clustering: {\em purity}, {\em normalized mutual information}, and the {\em rand
index}.

%%

To compute purity, each cluster is assigned to the class which is most frequent
in the cluster, and then the accuracy of this assignment is measured by counting
the number of correctly assigned observations.
%
Formally:

$$ \text{purity}(\Omega, \mathbb{C}) = \frac{1}{N} \sum_{k}^K \max_j |\omega_k \cap c_j|$$

where $\Omega = \{\omega_1, \omega_2, \ldots, \omega_K\}$ is the set of clusters
and $\mathbb{C} = \{c_1, c_2, \ldots, c_J\}$ is the set of classes (the gold
standard).

%%

Bad clustering solutions have values of purity close to 0, and perfect
clustering has a value of 1. 
%
However, as $K$ increases, the value of purity tends to increase as well. 
%
For example, a purity of 1 is achieved when every cluster has only one
observation.

%%

To make a trade-off between the number of clusters and the quality of the
clustering solution, we can use normalized mutual information, or NMI.
%
NMI is defined as follows:

$$ \text{NMI}(\Omega, \mathbb{C}) = \frac{I(\Omega; \mathbb{C})}{[H(\Omega) + H(\mathbb{C})]/2} $$

$I$ is mutual information, defined as 


\begin{align}
    I(\Omega; \mathbb{C}) &= \sum_k \sum_j P(\omega_k \cap c_j) \log \frac{P(\omega_k \cap c_j)}{P(\omega_k) P(\cap c_j)} \label{eq:mutual-info} \\ 
    & = \sum_k \sum_j \frac{|\omega_k \cap c_j|}{N} \log \frac{N|\omega_k \cap c_j|}{|\omega_k| |c_j|} \label{eq:mutual-info2}
\end{align}



where $P(\omega_k)$, $P(c_j)$, and $P(\omega_k \cap c_j)$ are the probabilities
of the observations being in cluster $\omega_k$, class $c_j$, and in the
intersection of $\omega_k$ and $c_j$, respectively. 
%
Equation~\ref{eq:mutual-info2} is equivalent to \ref{eq:mutual-info} for maximum
likelihood estimates of the probabilities, that is, the relative frequency.
%
$H$ is entropy, defined as 

\begin{align}
H(\Omega) &= - \sum_k P(\omega_k) \log P(\omega_k) \label{eq:entropy} \\
&= - \sum_k \frac{|\omega_k|}{N} \log \frac{|\omega_k|}{N} \label{eq:entropy2}
\end{align}
    
The intuition behind mutual information (MI) is that it measures the amount of
knowledge we gain about the classes when we know what the clusters are. 
%
When the clustering assignment is random, we gain no new knowledge, and MI is 0.
%
It suffers the same problem as purity, so MI is maximum when the perfect
clustering is also one cluster per observation.
%
For that, NMI normalizes MI using entropy.

%%

Entropy tends to increase with the number of clusters. 
%
In this case, NMI is low when $K = N$, because $H(\Omega)$ is maximum when $K =
N$. 
%
The denominator $[H(\Omega) + H(\mathbb{C})]/2$ is a tight upper bound on MI, so
NMI ranges between 0 and 1.

%%

Another view on clustering is to consider the process as a series of decisions,
where we decide for each pair of observations, if they belong to the same
cluster or not.
%
We want to assign observations to the same cluster if they are similar (or if
they both belong to the same class). 
%
We can define similarity as a score of a pair of observations, such that a high score
indicates that the two observations are {\em similar} in a sense, and a low score 
indicates that the two observations are {\em different}.
%
So, if we assign a pair of similar observations to the same cluster, it is
considered a True Positive (TP).
%
A pair of dissimilar observations assigned to different clusters is a True
Negative (TN).
%
Likewise, two similar observations in different clusters corresponds to a False
Negative (FN), and two dissimilar observations in the same cluster correspond to
a False Positive (FP).
%
The Rand Index (RI) measures the percentage of correct decisions, or just the
accuracy.

\begin{equation}
    \text{RI} = \frac{ \text{TP} + \text{TN} }{ \text{TP} + \text{FP} + \text{FN} + \text{TN}}
\end{equation}

From this representation we can define the equivalent to precision, recall, and
F-score.

\section{Neural Network-based Language Models}

A {\em language model} defines a probability distribution over sequences of
tokens in a natural language\footnote{This section is broadly based on the book
{\em Deep Learning} from Goodfellow et al.~\cite{Goodfellow-et-al-2016}}.
%
Neural Language Models are a class of language models designed to
overcome the {\em curse of dimensionality} problem for modeling natural language
sequences by using a distributed representation of words.
%
Neural Language Models are able to recognize that two words are similar without
losing the ability to encode each word as distinct from the other.
%
Neural Language Models also share statistical strength between one word (and its
context) and other similar words and contexts.
%
The distributed representation the model learns for each word enables this
sharing by allowing the model to treat words that have features in common
similarly.
%
For example, if the word {\tt dog} and {\tt cat} map to representations that
share many attributes, then sentences that contain {\tt cat} can inform the
predictions that will be made by the model for sentences that contain {\tt dog},
and vice versa.
%
The curse of dimensionality requires the model to generalize to a number of
sentences that is exponential in the sentence length.
%
The model counters this problem by relating each training sentence to an
exponential number of similar sentences.

%%

These word representations are often called {\em word embeddings}. 
%
In this interpretation, we view raw symbols as points in a space of dimension
equal to the vocabulary size.
%
The word representations embed those points in a feature space of lower dimension.
%
In the original space, each word is represented by a one-hot vector (that is, a
vector of the size of the vocabulary, where all dimensions are zero, except in
the position of the word itself, where it is 1), so every pair of words is at
Euclidean distance $\sqrt{2}$ from each other.
%
In the embedding space, words that appear in similar contexts (or any pair of
words sharing some `features' learned by the model), are close to each other.
%
This often results in words with similar meaning being neighbors.


\paragraph{Skipgram model~\cite{mikolov2013efficient}.}
%
Given a vocabulary of size $W$, where a word is identified by its index $w \in
\{1, \ldots, W\}$, the goal is to learn a vector representation of each word $w$.
%
The word representations are trained with the goal of predicting other words that
are in the same context.
%
Given a large sequence of words $w_1, w_2, \ldots, w_T$, the objective of the
skipgram model is to maximize the following log-likelihood:

$$ \sum_{t=1}^T\sum_{c\in \mathcal{C}_t} \log P(w_c | w_t) $$

\noindent where $\mathcal{C}_t$ is the context of $w_t$, the set of indices of
words surrounding $w_t$. 
%
The probability of observing a context word $w_c$ given $w_t$ will be
parameterized using the word vectors.

%%

Assume that we are given a scoring function $s$ which maps a pair of (word,
context) to values in $\mathbb{R}$.
%
One way to define the probability of a context word is using the {\em softmax},
$ P(w_c | w_t) = \frac{ \mathit{e}^{s(w_t, w_c)} }{ \sum_{j=1}^W \mathit{e}^{s(w_t, j)} } $.
%
There are two problems with this approach: one being the amount of output units
of the neural network ($W$ in this case, one for each word in the vocabulary),
which does not scale with the amount of training data required, and the other is
that it implies that, given a word $w_t$, we only predict one context word
$w_c$.

%%

In the original proposal, there are three ways to approximate the softmax:
%
hierarchical softmax,
%
Noise Contrastive Estimation, and
%
negative sampling.
%
We will describe negative sampling briefly.
% 
The problem of predicting context words can be framed as a set of independent
binary classification tasks.
%
The goal is then to predict the presence or the absence of context words. 
%
For the word at position $t$ we consider all context words as positive examples,
and sample negative examples at random from the vocabulary.
%
For a chosen context position $c$, and using the logistic loss for binary
classification, the log-likelihood is as follows:

$$ \log (1 + \mathit{e}^{-s(w_t, w_c)}) + \sum_{n\in \mathcal{N}_{t,c}} \log(1 + \mathit{e}^{s(w_t, n)})$$

\noindent where $\mathcal{N}_{t,c}$ is a set of negative examples sampled from
the vocabulary. 
%
By denoting the logistic loss function as $\ell : x \mapsto \log(1+\mathit{e}^{-x})$, we
can rewrite the objective function as:

$$\sum_{t=1}^T \Big[ \sum_{c\in\mathcal{C}_t} \ell(s(w_t, w_c)) + \sum_{n\in\mathcal{N}_{t,c}}\ell(-s(w_t, n))\Big]$$

Finally, the scoring function $s$ can be parameterized using the word vectors. 
%
For each word $w$ we define two vectors $\vect{u}_w$ and $\vect{v}_w$.
%
These two vectors are sometimes referred to as {\em input} and {\em output}
vectors, respectively.
%
The score is then computed as the scalar product between the vectors as $s(w_t,
w_c) = \vect{u}^\text{T}_{w_t}\vect{v}_{w_c}$.
%
This model is the skipgram model with negative sampling, introduced by Mikolov
et al.~\cite{mikolov2013distributed}.


\paragraph{fastText~\cite{bojanowski2017enriching}.} 
By using a distinct vector representation for each word, the skipgram model
ignores the internal structure of words. 
%
Some languages, such as French or Spanish, which contain several cases for
verbs or nouns, contain many word forms that occur rarely in the training
corpus, making it difficult to learn good word representations.
%
In the case of Twitter, the vocabulary is very large due to the usage of
uncommon or rare words (misspellings, new words, abbreviations, etc.).
%
For this, each word in the vocabulary is represented as a bag of character
n-grams. 
%
Two special symbols {\tt <} and {\tt >} are added to the beginning and end of
each word, respectively, to distinguish prefixes and suffixes from other
sequences.
%
Additionally, the word $w$ itself is added to the set of n-grams, to learn a
representation for the words as well. 
%
Suppose we are given a set of n-grams of size $G$. 
%
Given a word $w$, denote by $\mathcal{G}_w \subset \{1, \ldots, G\}$ the set of
n-grams appearing in $w$.
%
We associate a vector representation $\vect{z}_g$ to each n-gram $g$. 
%
We represent a word by the sum of the vector representations of its n-grams.
%
Thus, the new scoring function is

$$s(w, c) = \sum_{g\in\mathcal{G}_w}\vect{z}^\text{T}_g\vect{v}_c$$

This model allows us to share the representation across words, thus allowing us
to learn reliable representation of rare words.
%
This is the {\em fastText} model.